\chapter{Conclusion}

\section{Summary}

In this work a benchmarking suite for \ac{GASPI} applications is developed. This suite comprises of a series of microbenchmarks and one application benchmark. For measurement, common established techniques like warm-up runs, averaging multiple iterations, time aggregation on multiple nodes, optional block aggregation and automatically decreasing measurement iterations for large block sizes are utilized. Each implemented benchmark is discussed in detail.

For comparison with other \ac{PGAS} models the \ac{UPC} and \ac{OpenSHMEM} programming models were introduced and existing benchmarking suites to measure their performance were shown. In the evaluation of the developed benchmarking suite the performance of \ac{GASPI} using \ac{GPI}-2 was compared to the \ac{MPI} performance with Open MPI using four different kinds of interconnection networks. It was found that \ac{GPI}-2 performs similarly to \ac{MPI} with respect to bandwidth in the testing system that consists of only two nodes. Statements which claim that \ac{GPI}-2 performs significantly better than \ac{MPI} could not be confirmed with the given test setup. It shall, however, be noted that the testing setup that was used in this work differs from the benchmark conducted by the \ac{GPI}-2 developers. Namely, no InfiniBand FDR network was available, therefore only InfiniBand QDR performance could be measured.

The usage of \ac{GASPI} differs from \ac{MPI} in many ways: Especially the explicit number of communication queues and weak synchronization primitives with notifications are an alternative concept in distributed communication. Since the performance of \ac{GASPI} and \ac{MPI} is in the same regime, choosing the adequate programming model for an application becomes a user-specific matter of taste. A definitive drawback of using \ac{GASPI} is the extremely small public community support: There exists only one publicly available implementation of \ac{GASPI} maintained by a single group of developers. Special functions for communication like scatter, gather or broadcast that are available in \ac{MPI} are missing in \ac{GASPI} and need to be developed individually. On the one hand this makes the \ac{GASPI} functionality easier to comprehend in full but on the other hand hinders the propagation of \ac{GASPI}'s usage as the communication library of choice.

\section{Future Work}

The developed microbenchmarking suite measures the most important figures for popular communication schemes. Application benchmarks are only given as an example how \ac{GASPI} communication integrates into existing programs and how external benchmarks may be integrated into the benchmarking suite. For further evaluation it is extremely useful to integrate a larger set of application benchmarks into the developed suite. This especially includes porting the \ac{NPB} benchmarks to \ac{GASPI} which has not been done before. Doing this requires significant work since the majority of the \ac{NPB} applications are written in Fortran. Even though there exists a Fortran binding for \ac{GPI}-2, a re-write in C would probably be adequate.

Another path of work is to further comprehend the differences between the reference benchmarks provided on the \ac{GPI}-2 website and the actual measurements on a testing system \cite{gpi-2-site}. To pursue this, an InfiniBand FDR interface should be procured for the testing nodes in order to measure in the same regime of absolute values as the reference benchmarks. Furthermore, it may prove beneficial to compare the collective and atomic operation rate of \ac{MPI} to \ac{GASPI}. For this to produce expressive results, more than two nodes are required in the testing system. This is because the runtime of collective operations scales with the number of nodes in the synchronized group. For a comparison, the figure of merit is the scaling behavior of the execution latency of collectives and atomics with the number of nodes in the system.



