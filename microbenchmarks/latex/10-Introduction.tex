
\chapter{Introduction}

In computational science, \ac{HPC} is used to address large scale issues of high computational complexity. Applications encompass for example weather forecasts, disease research and aerodynamics simulations for vehicles or aircrafts. \Ac{HPC} systems are highly parallel and consist of numerous computing nodes which incorporate the required hardware to perform the computations itself as well as components to provide interconnection between the nodes. Modern \ac{HPC} clusters are heterogeneous: This means that there are different types of nodes which are either suited for \eg \acs{CPU}-intensive workloads or provide different kinds of accelerators like \acsp{GPU} or \acsp{FPGA} for specialized workloads. 

Because of the distributed nature of \ac{HPC} systems, the interconnection network that provides communication across the different nodes of the cluster is of par\-ti\-cu\-lar importance. On the one hand, adequate hardware components have to be developed that provide reliable high bandwidth communication with the lowest possible latency. On the other hand, software libraries, programming models and algorithms must complement the hardware to deliver a high performance communication solution to the respective application.

In this work the programming model \ac{GASPI} is presented which builds on the concepts of a \ac{PGAS}. Features of \ac{GASPI} are explained and put into context with the de facto standard \ac{MPI} which is used for high performance communication. In order to compare the performance of \ac{GASPI} to other established programming models and libraries, a benchmark suite is developed for \ac{GASPI} that encompasses microbenchmarks and application benchmarks. The benchmark suite is meant to extend existing benchmarks and give insights how to use \ac{GASPI} in productive program environments. 

\section{Motivation}

When new programming paradigms are established and programming libraries implementing these paradigms are developed, their performance must be evaluated. This is necessary in order to persuade scientist to use the new concepts and to substantiate advantages or possible pitfalls of using those new techniques. In computational science, the \ac{MPI} standard is the unchallenged reference for communication in distributed parallel programming. This is because \ac{MPI} was inspired by numerous different concepts to provide much more functionality than simple dual-sided message passing as the name originally suggests. The interface that the \ac{MPI} standard defines is implemented in various software libraries; the most prominent examples are Open MPI, MPICH and MVAPICH. Special implementations exist for proprietary communication hardware like \eg in Cray systems.

For evaluating \ac{MPI} implementations, known benchmark suites have been established. Especially the \ac{IMB} and the \ac{OMB} suites are widely used to compare different \ac{MPI} implementations or to evaluate the performance metrics of other programming models like \ac{OpenSHMEM} in comparison to \ac{MPI}. Both suites mentioned consist of a series of microbenchmarks. Microbenchmarks target the evaluation of the base functionality of the subject under test. Application benchmarks on the other hand utilize the communication primitives provided by the evaluated library in a typical use case application for high performance computing. The goal of application benchmarks is to employ characteristic communication patterns in the benchmark that are commonly found in real \ac{HPC} environments. Examples of those patterns are stencil codes with halo exchanges, producer/consumer patterns or embarrassingly parallel computations like matrix multiplications. 

Metrics to be gathered when running microbenchmarks are usually latency, bandwidth and operation rate. In most cases the operation latency is the fundamental metric that can be measured while bandwidth and operation rate are derived metrics. The achieved bandwidth of a benchmark is often compared to the maximum bandwidth of the employed interconnection network: For 10\,GBit/s Ethernet this would be approximately 10\,GBit/s or 1.25\,GB/s. Benchmarks of certain communication schemes are rerun for different transfer sizes to allow for efficiency estimates when using a given transfer method.

In contrast to \ac{MPI} being exhaustively benchmarked by numerous works, there exists only a small number of benchmarks for \ac{GASPI}. Apart from a selected number of microbechmarks delivered with the reference implementation \acs{GPI}-2 of the \ac{GASPI} standard, no benchmark suite is publicly available at the time of writing. Openly distributed benchmarks are the basis of scientific comparability of \ac{GASPI} with its alternatives and therefore a crucial factor for determining if \ac{GASPI} is suitable for a given project. The benchmarks developed in this work are similar to the \ac{IMB} and \ac{OMB} benchmarks to allow for a simple interpretation of results when benchmark data of the different suites is obtained.

\section{Related Work}

The objective of this work is to provide a benchmarking suite for the \acs{GASPI} programming model. For other related programming models, existing benchmarking suites are established. The de facto standard for communication in distributed programming is \acs{MPI}. Widely used benchmarks for \acs{MPI} are \acs{OMB} and \acs{IMB} \cite{imb, omb}.

In addition to \acs{MPI} benchmarks, \acs{OMB} also provides microbenchmarks for other \acs{PGAS} programming models such as \acs{OpenSHMEM} and \acs{UPC}. These models will be further explained in \chref{ch:background}.

A series of publicly available microbenchmarks for \acs{GASPI} only exist on the internet page of the \acs{GPI}-2 developers \cite{gpi-2-site}. A selection of few simple microbenchmarks along with an evaluation of application performance in comparison to \acs{MPI}-1 is provided in \cite{gaspi-heuveline}. Focus on the evaluation of the notified read benchmark of \acs{GASPI} is set in the works of V. \textsc{End} \cite{diss-end}. Apart from other application-related benchmarks, no works are available that specifically focus on \acs{GASPI} microbenchmarks.
